{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8. Recurrent Neural Networks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Recurrent Neural Networks\n"
      ],
      "metadata": {
        "id": "gyLEckMp3cYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1. Sequence Models\n"
      ],
      "metadata": {
        "id": "c0JqeFav3jRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1.2. Training\n"
      ],
      "metadata": {
        "id": "owYodlH9P430"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l==0.14.2"
      ],
      "metadata": {
        "id": "GDKKm5YBP68w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3RebSJZ3It8"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "T = 1000  # Generate a total of 1000 points\n",
        "time = torch.arange(1, T + 1, dtype=torch.float32)\n",
        "x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))\n",
        "d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tau = 4\n",
        "features = torch.zeros((T - tau, tau))\n",
        "for i in range(tau):\n",
        "    features[:, i] = x[i: T - tau + i]\n",
        "labels = x[tau:].reshape((-1, 1))\n",
        "\n",
        "batch_size, n_train = 16, 600\n",
        "# Only the first `n_train` examples are used for training\n",
        "train_iter = d2l.load_array((features[:n_train], labels[:n_train]),\n",
        "                            batch_size, is_train=True)"
      ],
      "metadata": {
        "id": "DV6gkdJBQEfV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for initializing the weights of the network\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "# A simple MLP\n",
        "def get_net():\n",
        "    net = nn.Sequential(nn.Linear(4, 10),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(10, 1))\n",
        "    net.apply(init_weights)\n",
        "    return net\n",
        "\n",
        "# Note: `MSELoss` computes squared error without the 1/2 factor\n",
        "loss = nn.MSELoss(reduction='none')"
      ],
      "metadata": {
        "id": "uJqX-KgCR_fs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, train_iter, loss, epochs, lr):\n",
        "    trainer = torch.optim.Adam(net.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        for X, y in train_iter:\n",
        "            trainer.zero_grad()\n",
        "            l = loss(net(X), y)\n",
        "            l.sum().backward()\n",
        "            trainer.step()\n",
        "        print(f'epoch {epoch + 1}, '\n",
        "              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')\n",
        "\n",
        "net = get_net()\n",
        "train(net, train_iter, loss, 5, 0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxNdITgTSA-Z",
        "outputId": "477d6474-aa19-46f5-b51b-c5edb9589a54"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, loss: 0.095671\n",
            "epoch 2, loss: 0.059866\n",
            "epoch 3, loss: 0.057206\n",
            "epoch 4, loss: 0.054973\n",
            "epoch 5, loss: 0.054723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1.3. Prediction\n"
      ],
      "metadata": {
        "id": "tz-sdmD7SPNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onestep_preds = net(features)\n",
        "d2l.plot([time, time[tau:]], [x.detach().numpy(), onestep_preds.detach().numpy()], 'time',\n",
        "         'x', legend=['data', '1-step preds'], xlim=[1, 1000], figsize=(6, 3))"
      ],
      "metadata": {
        "id": "5WbJRalGSB8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multistep_preds = torch.zeros(T)\n",
        "multistep_preds[: n_train + tau] = x[: n_train + tau]\n",
        "for i in range(n_train + tau, T):\n",
        "    multistep_preds[i] = net(\n",
        "        multistep_preds[i - tau:i].reshape((1, -1)))\n",
        "\n",
        "d2l.plot([time, time[tau:], time[n_train + tau:]],\n",
        "         [x.detach().numpy(), onestep_preds.detach().numpy(),\n",
        "          multistep_preds[n_train + tau:].detach().numpy()], 'time',\n",
        "         'x', legend=['data', '1-step preds', 'multistep preds'],\n",
        "         xlim=[1, 1000], figsize=(6, 3))"
      ],
      "metadata": {
        "id": "gZ9U9Mq3SSWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 64\n",
        "\n",
        "features = torch.zeros((T - tau - max_steps + 1, tau + max_steps))\n",
        "# Column `i` (`i` < `tau`) are observations from `x` for time steps from\n",
        "# `i + 1` to `i + T - tau - max_steps + 1`\n",
        "for i in range(tau):\n",
        "    features[:, i] = x[i: i + T - tau - max_steps + 1]\n",
        "\n",
        "# Column `i` (`i` >= `tau`) are the (`i - tau + 1`)-step-ahead predictions for\n",
        "# time steps from `i + 1` to `i + T - tau - max_steps + 1`\n",
        "for i in range(tau, tau + max_steps):\n",
        "    features[:, i] = net(features[:, i - tau:i]).reshape(-1)\n",
        "\n",
        "steps = (1, 4, 16, 64)\n",
        "d2l.plot([time[tau + i - 1: T - max_steps + i] for i in steps],\n",
        "         [features[:, (tau + i - 1)].detach().numpy() for i in steps], 'time', 'x',\n",
        "         legend=[f'{i}-step preds' for i in steps], xlim=[5, 1000],\n",
        "         figsize=(6, 3))"
      ],
      "metadata": {
        "id": "AOHoRt7fSTjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2. Text Preprocessing\n"
      ],
      "metadata": {
        "id": "VJzlIaImUmmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import re"
      ],
      "metadata": {
        "id": "EwN2BXPESUrx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2.1. Reading the Dataset\n"
      ],
      "metadata": {
        "id": "4XR7ZzNjUrO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n",
        "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
        "\n",
        "def read_time_machine():  \n",
        "    with open(d2l.download('time_machine'), 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
        "\n",
        "lines = read_time_machine()\n",
        "print(f'# text lines: {len(lines)}')\n",
        "print(lines[0])\n",
        "print(lines[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tp0mRZA6UqQP",
        "outputId": "2f053580-4a70-4b1b-c04b-9912c4dff86b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# text lines: 3221\n",
            "the time machine by h g wells\n",
            "twinkled and his usually pale face was flushed and animated the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2.2. Tokenization\n"
      ],
      "metadata": {
        "id": "XGHkYEGxUwK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lines, token='word'):  \n",
        "    if token == 'word':\n",
        "        return [line.split() for line in lines]\n",
        "    elif token == 'char':\n",
        "        return [list(line) for line in lines]\n",
        "    else:\n",
        "        print('ERROR: unknown token type: ' + token)\n",
        "\n",
        "tokens = tokenize(lines)\n",
        "for i in range(11):\n",
        "    print(tokens[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrzSFyhuUwFP",
        "outputId": "f1a20526-cd97-47fc-e5aa-3ffc0f76ced3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "['i']\n",
            "[]\n",
            "[]\n",
            "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
            "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
            "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2.3. Vocabulary\n"
      ],
      "metadata": {
        "id": "tEVWKxilU4AC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab: \n",
        "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "        if tokens is None:\n",
        "            tokens = []\n",
        "        if reserved_tokens is None:\n",
        "            reserved_tokens = []\n",
        "        # Sort according to frequencies\n",
        "        counter = count_corpus(tokens)\n",
        "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                   reverse=True)\n",
        "        # The index for the unknown token is 0\n",
        "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
        "        self.token_to_idx = {token: idx\n",
        "                             for idx, token in enumerate(self.idx_to_token)}\n",
        "        for token, freq in self._token_freqs:\n",
        "            if freq < min_freq:\n",
        "                break\n",
        "            if token not in self.token_to_idx:\n",
        "                self.idx_to_token.append(token)\n",
        "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if not isinstance(indices, (list, tuple)):\n",
        "            return self.idx_to_token[indices]\n",
        "        return [self.idx_to_token[index] for index in indices]\n",
        "\n",
        "    @property\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return 0\n",
        "\n",
        "    @property\n",
        "    def token_freqs(self):  # Index for the unknown token\n",
        "        return self._token_freqs\n",
        "\n",
        "def count_corpus(tokens):  \n",
        "    # Here `tokens` is a 1D list or 2D list\n",
        "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "        # Flatten a list of token lists into a list of tokens\n",
        "        tokens = [token for line in tokens for token in line]\n",
        "    return collections.Counter(tokens)"
      ],
      "metadata": {
        "id": "9-awq9lRUtwe"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocab(tokens)\n",
        "print(list(vocab.token_to_idx.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWiHdIbjU7A4",
        "outputId": "7f05e2e3-3ef7-4c9a-fcff-be66d8a7685b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [0, 10]:\n",
        "    print('words:', tokens[i])\n",
        "    print('indices:', vocab[tokens[i]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8TXrc68U9A6",
        "outputId": "e5c31f87-1fea-4810-9dfc-875bb3f3d9aa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
            "indices: [1, 19, 50, 40, 2183, 2184, 400]\n",
            "words: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
            "indices: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2.4. Putting All Things Together\n"
      ],
      "metadata": {
        "id": "CbSdsA9jU_b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_corpus_time_machine(max_tokens=-1): \n",
        "    lines = read_time_machine()\n",
        "    tokens = tokenize(lines, 'char')\n",
        "    vocab = Vocab(tokens)\n",
        "    # Since each text line in the time machine dataset is not necessarily a\n",
        "    # sentence or a paragraph, flatten all the text lines into a single list\n",
        "    corpus = [vocab[token] for line in tokens for token in line]\n",
        "    if max_tokens > 0:\n",
        "        corpus = corpus[:max_tokens]\n",
        "    return corpus, vocab\n",
        "\n",
        "corpus, vocab = load_corpus_time_machine()\n",
        "len(corpus), len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8mfwtx_U-U_",
        "outputId": "a9178e5e-e8e8-492f-d4c6-a85f69b1af89"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(170580, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3. Language Models and the Dataset\n"
      ],
      "metadata": {
        "id": "x76Y-YWxVEhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3.3. Natural Language Statistics¶\n"
      ],
      "metadata": {
        "id": "_VlnymI2VIdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "tokens = d2l.tokenize(d2l.read_time_machine())\n",
        "# Since each text line is not necessarily a sentence or a paragraph, we\n",
        "# concatenate all text lines\n",
        "corpus = [token for line in tokens for token in line]\n",
        "vocab = d2l.Vocab(corpus)\n",
        "vocab.token_freqs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqMJg6m_VA5g",
        "outputId": "7ca3366e-f6b1-47d5-905d-fa93a2899c1f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('e', 17838),\n",
              " ('t', 13515),\n",
              " ('a', 11704),\n",
              " ('i', 10138),\n",
              " ('n', 9917),\n",
              " ('o', 9758),\n",
              " ('s', 8486),\n",
              " ('h', 8257),\n",
              " ('r', 7674),\n",
              " ('d', 6337)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freqs = [freq for token, freq in vocab.token_freqs]\n",
        "d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',\n",
        "         xscale='log', yscale='log')"
      ],
      "metadata": {
        "id": "MKFDXVXsVKZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]\n",
        "bigram_vocab = d2l.Vocab(bigram_tokens)\n",
        "bigram_vocab.token_freqs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUw3x5-uVLWU",
        "outputId": "cb72e155-9114-41a9-dca2-f7039310f005"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 4521),\n",
              " ('', 2563),\n",
              " ('i', 2534),\n",
              " ('and', 2490),\n",
              " ('of', 2310),\n",
              " ('a', 1632),\n",
              " ('to', 1390),\n",
              " ('was', 1104),\n",
              " ('in', 1082),\n",
              " ('that', 886)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_tokens = [triple for triple in zip(\n",
        "    corpus[:-2], corpus[1:-1], corpus[2:])]\n",
        "trigram_vocab = d2l.Vocab(trigram_tokens)\n",
        "trigram_vocab.token_freqs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCOQTXL9VMqU",
        "outputId": "63ff58b3-c1f8-4e57-a1ff-d8a3960f8a8c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 6781),\n",
              " ('', 3844),\n",
              " ('i', 3801),\n",
              " ('and', 3735),\n",
              " ('of', 3465),\n",
              " ('a', 2448),\n",
              " ('to', 2085),\n",
              " ('was', 1656),\n",
              " ('in', 1623),\n",
              " ('that', 1329)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]\n",
        "trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]\n",
        "d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',\n",
        "         ylabel='frequency: n(x)', xscale='log', yscale='log',\n",
        "         legend=['unigram', 'bigram', 'trigram'])"
      ],
      "metadata": {
        "id": "H1V7WqUPVOGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3.4.1. Random Sampling\n",
        "\n"
      ],
      "metadata": {
        "id": "f_oZlx5kVSVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_data_iter_random(corpus, batch_size, num_steps):  \n",
        "    # Start with a random offset (inclusive of `num_steps - 1`) to partition a\n",
        "    # sequence\n",
        "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
        "    # Subtract 1 since we need to account for labels\n",
        "    num_subseqs = (len(corpus) - 1) // num_steps\n",
        "    # The starting indices for subsequences of length `num_steps`\n",
        "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "    # In random sampling, the subsequences from two adjacent random\n",
        "    # minibatches during iteration are not necessarily adjacent on the\n",
        "    # original sequence\n",
        "    random.shuffle(initial_indices)\n",
        "\n",
        "    def data(pos):\n",
        "        # Return a sequence of length `num_steps` starting from `pos`\n",
        "        return corpus[pos: pos + num_steps]\n",
        "\n",
        "    num_batches = num_subseqs // batch_size\n",
        "    for i in range(0, batch_size * num_batches, batch_size):\n",
        "        # Here, `initial_indices` contains randomized starting indices for\n",
        "        # subsequences\n",
        "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
        "        X = [data(j) for j in initial_indices_per_batch]\n",
        "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
        "        yield torch.tensor(X), torch.tensor(Y)"
      ],
      "metadata": {
        "id": "DWExIKLOVPGG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_seq = list(range(35))\n",
        "for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n",
        "    print('X: ', X, '\\nY:', Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo3KBUr6VWRz",
        "outputId": "addc2472-1388-4981-a963-1e09ec7557af"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:  tensor([[19, 20, 21, 22, 23],\n",
            "        [29, 30, 31, 32, 33]]) \n",
            "Y: tensor([[20, 21, 22, 23, 24],\n",
            "        [30, 31, 32, 33, 34]])\n",
            "X:  tensor([[14, 15, 16, 17, 18],\n",
            "        [ 9, 10, 11, 12, 13]]) \n",
            "Y: tensor([[15, 16, 17, 18, 19],\n",
            "        [10, 11, 12, 13, 14]])\n",
            "X:  tensor([[ 4,  5,  6,  7,  8],\n",
            "        [24, 25, 26, 27, 28]]) \n",
            "Y: tensor([[ 5,  6,  7,  8,  9],\n",
            "        [25, 26, 27, 28, 29]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3.4.2. Sequential Partitioning\n"
      ],
      "metadata": {
        "id": "1aX5JzfYVZjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_data_iter_sequential(corpus, batch_size, num_steps): \n",
        "    # Start with a random offset to partition a sequence\n",
        "    offset = random.randint(0, num_steps)\n",
        "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
        "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
        "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
        "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
        "    num_batches = Xs.shape[1] // num_steps\n",
        "    for i in range(0, num_steps * num_batches, num_steps):\n",
        "        X = Xs[:, i: i + num_steps]\n",
        "        Y = Ys[:, i: i + num_steps]\n",
        "        yield X, Y"
      ],
      "metadata": {
        "id": "ogN3ARruVYSs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n",
        "    print('X: ', X, '\\nY:', Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTYGgw0VVc-H",
        "outputId": "e2ccd022-9e50-4b6b-e68d-b0d250b9dfe0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:  tensor([[ 1,  2,  3,  4,  5],\n",
            "        [17, 18, 19, 20, 21]]) \n",
            "Y: tensor([[ 2,  3,  4,  5,  6],\n",
            "        [18, 19, 20, 21, 22]])\n",
            "X:  tensor([[ 6,  7,  8,  9, 10],\n",
            "        [22, 23, 24, 25, 26]]) \n",
            "Y: tensor([[ 7,  8,  9, 10, 11],\n",
            "        [23, 24, 25, 26, 27]])\n",
            "X:  tensor([[11, 12, 13, 14, 15],\n",
            "        [27, 28, 29, 30, 31]]) \n",
            "Y: tensor([[12, 13, 14, 15, 16],\n",
            "        [28, 29, 30, 31, 32]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqDataLoader:  \n",
        "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
        "        if use_random_iter:\n",
        "            self.data_iter_fn = d2l.seq_data_iter_random\n",
        "        else:\n",
        "            self.data_iter_fn = d2l.seq_data_iter_sequential\n",
        "        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)\n",
        "        self.batch_size, self.num_steps = batch_size, num_steps\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
      ],
      "metadata": {
        "id": "FxQA4OG_VdMI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_time_machine(batch_size, num_steps,  \n",
        "                           use_random_iter=False, max_tokens=10000):\n",
        "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
        "    data_iter = SeqDataLoader(\n",
        "        batch_size, num_steps, use_random_iter, max_tokens)\n",
        "    return data_iter, data_iter.vocab"
      ],
      "metadata": {
        "id": "mnZm7BXSVmI-"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.4. Recurrent Neural Networks\n"
      ],
      "metadata": {
        "id": "m7BY66swVsDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4.2. Recurrent Neural Networks with Hidden States\n"
      ],
      "metadata": {
        "id": "DrBqubQtVvhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4))\n",
        "H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))\n",
        "torch.matmul(X, W_xh) + torch.matmul(H, W_hh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64AaCsMVVoAe",
        "outputId": "d33bce0c-096a-43cb-9cb8-626dbf521cbc"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-3.3066, -0.9095,  1.9237,  3.1734],\n",
              "        [-0.3292, -0.5561,  0.7836, -0.6327],\n",
              "        [-6.8516, -0.0331,  1.5482, -0.5966]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.5. Implementation of Recurrent Neural Networks from Scratch\n"
      ],
      "metadata": {
        "id": "knPjfetqWAWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
      ],
      "metadata": {
        "id": "JhEbavEQWDxp"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5.1. One-Hot Encoding\n"
      ],
      "metadata": {
        "id": "jVajygG_WMUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "F.one_hot(torch.tensor([0, 2]), len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg65LJ4tWGNf",
        "outputId": "1a4fef26-5234-4a44-bcf5-0ed8efeb4fa6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0],\n",
              "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(10).reshape((2, 5))\n",
        "F.one_hot(X.T, 28).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCfi1TweWN6c",
        "outputId": "76e1d88b-af9d-4177-eebc-af7b9792dd8a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 2, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5.2. Initializing the Model Parameters\n"
      ],
      "metadata": {
        "id": "mzm4tu07WSOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_params(vocab_size, num_hiddens, device):\n",
        "    num_inputs = num_outputs = vocab_size\n",
        "\n",
        "    def normal(shape):\n",
        "        return torch.randn(size=shape, device=device) * 0.01\n",
        "\n",
        "    # Hidden layer parameters\n",
        "    W_xh = normal((num_inputs, num_hiddens))\n",
        "    W_hh = normal((num_hiddens, num_hiddens))\n",
        "    b_h = torch.zeros(num_hiddens, device=device)\n",
        "    # Output layer parameters\n",
        "    W_hq = normal((num_hiddens, num_outputs))\n",
        "    b_q = torch.zeros(num_outputs, device=device)\n",
        "    # Attach gradients\n",
        "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
        "    for param in params:\n",
        "        param.requires_grad_(True)\n",
        "    return params"
      ],
      "metadata": {
        "id": "43aAj6wgWO_9"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5.3. RNN Model¶\n"
      ],
      "metadata": {
        "id": "g3ue3WwPWVmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_rnn_state(batch_size, num_hiddens, device):\n",
        "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
      ],
      "metadata": {
        "id": "OwWKv0CpWT4-"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn(inputs, state, params):\n",
        "    # Here `inputs` shape: (`num_steps`, `batch_size`, `vocab_size`)\n",
        "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
        "    H, = state\n",
        "    outputs = []\n",
        "    # Shape of `X`: (`batch_size`, `vocab_size`)\n",
        "    for X in inputs:\n",
        "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
        "        Y = torch.mm(H, W_hq) + b_q\n",
        "        outputs.append(Y)\n",
        "    return torch.cat(outputs, dim=0), (H,)"
      ],
      "metadata": {
        "id": "wLhUQ11VWXsy"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModelScratch: \n",
        "    def __init__(self, vocab_size, num_hiddens, device,\n",
        "                 get_params, init_state, forward_fn):\n",
        "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
        "        self.params = get_params(vocab_size, num_hiddens, device)\n",
        "        self.init_state, self.forward_fn = init_state, forward_fn\n",
        "\n",
        "    def __call__(self, X, state):\n",
        "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
        "        return self.forward_fn(X, state, self.params)\n",
        "\n",
        "    def begin_state(self, batch_size, device):\n",
        "        return self.init_state(batch_size, self.num_hiddens, device)"
      ],
      "metadata": {
        "id": "gFlTnj4JWYnl"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_hiddens = 512\n",
        "net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,\n",
        "                      init_rnn_state, rnn)\n",
        "state = net.begin_state(X.shape[0], d2l.try_gpu())\n",
        "Y, new_state = net(X.to(d2l.try_gpu()), state)\n",
        "Y.shape, len(new_state), new_state[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7U6MfktWdTj",
        "outputId": "a87f7608-4343-4c84-9b66-cd6a4cc3d69e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 28]), 1, torch.Size([2, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5.4. Prediction"
      ],
      "metadata": {
        "id": "WT9uCdI-WiAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ch8(prefix, num_preds, net, vocab, device): \n",
        "    state = net.begin_state(batch_size=1, device=device)\n",
        "    outputs = [vocab[prefix[0]]]\n",
        "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
        "    for y in prefix[1:]:  # Warm-up period\n",
        "        _, state = net(get_input(), state)\n",
        "        outputs.append(vocab[y])\n",
        "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
        "        y, state = net(get_input(), state)\n",
        "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
        "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ],
      "metadata": {
        "id": "ZNxczFyPWd2W"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_ch8('time traveller ', 10, net, vocab, d2l.try_gpu())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mam2QdVhWlM0",
        "outputId": "479e98ce-a488-4fdc-d6f0-1770d86e7bf3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'time traveller hsoryhsory'"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5.5. Gradient Clipping"
      ],
      "metadata": {
        "id": "NtQBcM9vWpqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_clipping(net, theta): \n",
        "    if isinstance(net, nn.Module):\n",
        "        params = [p for p in net.parameters() if p.requires_grad]\n",
        "    else:\n",
        "        params = net.params\n",
        "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "    if norm > theta:\n",
        "        for param in params:\n",
        "            param.grad[:] *= theta / norm"
      ],
      "metadata": {
        "id": "LlDzHlq5WnLu"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5.6. Training"
      ],
      "metadata": {
        "id": "cxF_0tLJWwSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
        "    \"\"\"Train a net within one epoch (defined in Chapter 8).\"\"\"\n",
        "    state, timer = None, d2l.Timer()\n",
        "    metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "    for X, Y in train_iter:\n",
        "        if state is None or use_random_iter:\n",
        "            # Initialize `state` when either it is the first iteration or\n",
        "            # using random sampling\n",
        "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
        "        else:\n",
        "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
        "                # `state` is a tensor for `nn.GRU`\n",
        "                state.detach_()\n",
        "            else:\n",
        "                # `state` is a tuple of tensors for `nn.LSTM` and\n",
        "                # for our custom scratch implementation\n",
        "                for s in state:\n",
        "                    s.detach_()\n",
        "        y = Y.T.reshape(-1)\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_hat, state = net(X, state)\n",
        "        l = loss(y_hat, y.long()).mean()\n",
        "        if isinstance(updater, torch.optim.Optimizer):\n",
        "            updater.zero_grad()\n",
        "            l.backward()\n",
        "            grad_clipping(net, 1)\n",
        "            updater.step()\n",
        "        else:\n",
        "            l.backward()\n",
        "            grad_clipping(net, 1)\n",
        "            # Since the `mean` function has been invoked\n",
        "            updater(batch_size=1)\n",
        "        metric.add(l * y.numel(), y.numel())\n",
        "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
      ],
      "metadata": {
        "id": "QyaJP9v2WswT"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
        "              use_random_iter=False):\n",
        "    \"\"\"Train a model (defined in Chapter 8).\"\"\"\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
        "                            legend=['train'], xlim=[10, num_epochs])\n",
        "    # Initialize\n",
        "    if isinstance(net, nn.Module):\n",
        "        updater = torch.optim.SGD(net.parameters(), lr)\n",
        "    else:\n",
        "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
        "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
        "    # Train and predict\n",
        "    for epoch in range(num_epochs):\n",
        "        ppl, speed = train_epoch_ch8(\n",
        "            net, train_iter, loss, updater, device, use_random_iter)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(predict('time traveller'))\n",
        "            animator.add(epoch + 1, [ppl])\n",
        "    print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n",
        "    print(predict('time traveller'))\n",
        "    print(predict('traveller'))"
      ],
      "metadata": {
        "id": "3Bxklkc6WzwH"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr = 500, 1\n",
        "train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())"
      ],
      "metadata": {
        "id": "HoCAUvv2W1Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,\n",
        "                      init_rnn_state, rnn)\n",
        "train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),\n",
        "          use_random_iter=True)"
      ],
      "metadata": {
        "id": "vVrv7T_yW3nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.6. Concise Implementation of Recurrent Neural Networks"
      ],
      "metadata": {
        "id": "uLtS9CMhW7ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
      ],
      "metadata": {
        "id": "iJL0JvAXW4-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.6.1. Defining the Model\n",
        "\n"
      ],
      "metadata": {
        "id": "yqrtLFRQXBbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_hiddens = 256\n",
        "rnn_layer = nn.RNN(len(vocab), num_hiddens)"
      ],
      "metadata": {
        "id": "5_NaMYPyXAPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = torch.zeros((1, batch_size, num_hiddens))\n",
        "state.shape"
      ],
      "metadata": {
        "id": "u92oJT88XFNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(size=(num_steps, batch_size, len(vocab)))\n",
        "Y, state_new = rnn_layer(X, state)\n",
        "Y.shape, state_new.shape"
      ],
      "metadata": {
        "id": "COEnlRUJXGOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"The RNN model.\"\"\"\n",
        "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
        "        super(RNNModel, self).__init__(**kwargs)\n",
        "        self.rnn = rnn_layer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_hiddens = self.rnn.hidden_size\n",
        "        # If the RNN is bidirectional (to be introduced later),\n",
        "        # `num_directions` should be 2, else it should be 1.\n",
        "        if not self.rnn.bidirectional:\n",
        "            self.num_directions = 1\n",
        "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
        "        else:\n",
        "            self.num_directions = 2\n",
        "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
        "\n",
        "    def forward(self, inputs, state):\n",
        "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
        "        X = X.to(torch.float32)\n",
        "        Y, state = self.rnn(X, state)\n",
        "        # The fully connected layer will first change the shape of `Y` to\n",
        "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
        "        # (`num_steps` * `batch_size`, `vocab_size`).\n",
        "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
        "        return output, state\n",
        "\n",
        "    def begin_state(self, device, batch_size=1):\n",
        "        if not isinstance(self.rnn, nn.LSTM):\n",
        "            # `nn.GRU` takes a tensor as hidden state\n",
        "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
        "                                 batch_size, self.num_hiddens),\n",
        "                                device=device)\n",
        "        else:\n",
        "            # `nn.LSTM` takes a tuple of hidden states\n",
        "            return (torch.zeros((\n",
        "                self.num_directions * self.rnn.num_layers,\n",
        "                batch_size, self.num_hiddens), device=device),\n",
        "                    torch.zeros((\n",
        "                        self.num_directions * self.rnn.num_layers,\n",
        "                        batch_size, self.num_hiddens), device=device))"
      ],
      "metadata": {
        "id": "ylSTk52-XHcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.6.2. Training and Predicting"
      ],
      "metadata": {
        "id": "zWBdN9jbXLqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = d2l.try_gpu()\n",
        "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
        "net = net.to(device)\n",
        "d2l.predict_ch8('time traveller', 10, net, vocab, device)"
      ],
      "metadata": {
        "id": "aBJTrUgHXJ3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr = 500, 1\n",
        "d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)"
      ],
      "metadata": {
        "id": "WhaMMmvTXNjD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}